---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# ockhamSEM

## Overview

ockhamSEM is an R package for studying the fit propensity of single-group structural equation models with continuous items. Underlying support is through the [lavaan](https://lavaan.ugent.be/) package. A variety built-in graphical and text summaries are provided.

The package is introduced in Falk and Muthukrishna (2020) Parsimony in Model Selection: Tools for Assessing Fit Propensity.

This version of ockhamSEM includes alternative graph and table outputs.

## Installation

```{r, eval = FALSE}
# From GitHub:
# install.packages("devtools")
devtools::install_github("winterstat/ockhamSEM")
```

## Usage

Let's compare fit propensity for the following two models consisting of 3 variables:

![Model figure](man/figures/README-3varexample.png)

### 1. Set up a covariance matrix to fit models to:

```{r, eval = TRUE, message = FALSE, warning = FALSE}
library(ockhamSEM)

p<-3 # number of variables
temp_mat <- diag(p) # identity matrix
colnames(temp_mat) <- rownames(temp_mat) <- paste0("V", seq(1, p))
```

### 2. Define and fit the two models to be compared using the lavaan package:

```{r, eval = TRUE, message = FALSE, warning = FALSE}
mod1a <- 'V3 ~ V1 + V2
  V1 ~~ 0*V2'
mod2a <- 'V3 ~ V1
  V2 ~ V3'

mod1a.fit <- sem(mod1a, sample.cov=temp_mat, sample.nobs=500)
mod2a.fit <- sem(mod2a, sample.cov=temp_mat, sample.nobs=500)
```

### 3. Run fit propensity analysis

Here we use the onion method to generate random correlation matrices and will compare fit propensity for the SRMR and CFI fit measures.

```{r, eval = TRUE, message = FALSE, warning = FALSE}
res <- run.fitprop(mod1a.fit, mod2a.fit, fit.measure=c("srmr","cfi"),
  rmethod="onion",reps=100)
```

### 4. Summarize and plot fit propensity

#### Summarize:

```{r, eval = TRUE, message = FALSE, warning = FALSE}
summary(res)
```

#### Summarize intersections of FP:

The tables below show the number of datasets and proportion of data space that are fit well by different combinations of models. Here, 'None' reflects the part of the data space that was not fit well by any of the models.

```{r, eval = TRUE, message = FALSE, warning = FALSE}
intersect.fitprop(res, cutoff = c(.09, .95), lower.tail = c(TRUE, FALSE))
```

#### Graph Output:

There are several options for visualizing the fitting propensity analysis results. First, we can look at the ECDF plot comparing the cumulative densities of the fit indices across models.

```{r, eval = TRUE}
plot(res, type = "ecdf", whichfit = "cfi", lower.tail = FALSE)
```

We can also request a Euler plot, which shows what percentage of the data space is fit well by each model (based on a specific cutoff value), and to what extent different models' FP overlap.

```{r, eval = TRUE}
plot(res, type = "euler", whichfit = "srmr", cutoff = .09, lower.tail = TRUE)
```
In some cases, it may be important to see if the ranking of models in terms of their FP depends on the specific cutoff value used. The 'rank' plot compares the models' ranking across the range of the fit index value. Here, we can see that Model 2 always has higher FP than Model 1.

```{r, eval = TRUE}
plot(res, type = "rank", whichfit = "srmr", cutoff = .09, lower.tail = TRUE)
```

Finally, we can request a pairwise plot to see how large of a difference in FP (in terms of proportion) exists between two models across the range of the fit index.

```{r, eval = TRUE}
plot(res, type = "pairwise", whichmod = c(1,2), whichfit = "srmr", cutoff = .09, lower.tail = TRUE)
```
